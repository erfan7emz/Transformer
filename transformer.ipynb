{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHZhkDjZEV-5"
      },
      "source": [
        "# CS 480/680 assignment 3\n",
        "\n",
        "Tips:\n",
        "- Please save a copy of this notebook to avoid losing your changes.\n",
        "- Debug your code and ensure that it can run.\n",
        "- Save the output of each cell. Failure to do so may result in your coding questions not being graded.\n",
        "- To accelerate the training time, you can choose 'Runtime' -> 'Change runtime type' -> 'Hardware accelerator' and set 'Hardware accelerator' to 'GPU'.\n",
        "- Your grade is independent of the accuracy of your models. Your grade will depend on the correctness of your code and implementation.\n",
        "\n",
        "Tips for sumbission:\n",
        "- Do not change the order of the problems.\n",
        "- Select 'Runtime' -> 'Run all' to run all cells and generate a final \"gradable\" version of your notebook and save your ipynb file.\n",
        "- Also use 'File' -> 'Print' and then print your report from your browser into a PDF file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK9gmfL4KxeF"
      },
      "source": [
        "## Question 1 - Implementing GAN for MNIST dataset (35 points)\n",
        "\n",
        "In this question we are going to impelement a generative network that can generate MNIST handwritten digits from random Gaussian noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n-Mg5YdE5YR"
      },
      "source": [
        "### Q1.1 (2 points)\n",
        "\n",
        "We start by creating the data loaders. Note that each MNIST image is of size 28\\*28. The **first task** is to build the ```data_loader```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z-BU1m9IEXKY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5beccdf9-237d-479e-e331-bb172f4b1e82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 143770549.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 32371911.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 51353595.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3611474.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "# Setting the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "batch_size = 100\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "\n",
        "#ToDo: Build the train data loader using the above batch_size varibale and shuffling the dataset.\n",
        "data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_-NwpdfFGQs"
      },
      "source": [
        "### Q1.2 Generator (5 points)\n",
        "We will be building simple Generator and Discriminators that only consist of fully-connected layers. The Generator takes as input a Gaussian vector with dimension ```n_dim```. It consists of two linear layers with ```256``` and ```512``` nodes and the output layer should of dimension ```28*28=784``` so that it can be considered as unfolding a ```28*28``` image into a vector. The **second task** is to complete the following code for building the Generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uACYmlwwG2tL"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, n_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        #ToDo: Complete the code\n",
        "        #The activation function for the two hidden layers is ReLU.\n",
        "        #The last layer's activation function is Tanh.\n",
        "        self.fc1 = nn.Linear(n_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 512)\n",
        "        self.fc3 = nn.Linear(512, 784)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #ToDo: Complete this function. The function takes as input x and outputs the result of applying the generator on x.\n",
        "\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.tanh(self.fc3(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InnZY0glITeK"
      },
      "source": [
        "### Q1.3 Discriminator (5 points)\n",
        "The Discriminator takes as input an image of size ```28*28```. It consists of two linear layers with ```512``` and ```256``` nodes and the output layer should be a single node. The **third task** complete the following code for building the Discriminator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t5hdsi10JWJI"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "            #ToDo: Complete the code\n",
        "            #The activation functions for the two hidden layers are ReLU.\n",
        "            #The last layer's activation function is Sigmoid.\n",
        "        self.fc1 = nn.Linear(784, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1) # Making sure that the batch of images has the shape [batch_size,28*28] instead of [batch_size,1,28,28]\n",
        "        #ToDo: Complete this function. The function takes as input x and outputs the result of applying the generator on x.\n",
        "\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXItGnRYK69p"
      },
      "source": [
        "### Q1.4 Initializaiton (2 points)\n",
        "\n",
        "The **fourth task** is to define the optimizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FyuaaCJLLCdE"
      },
      "outputs": [],
      "source": [
        "n_dim = 100\n",
        "generator = Generator(n_dim).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "lr = 0.0002\n",
        "\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "#ToDo: define the two different Adam optimizers for generator and discriminator with learning rates of lr.\n",
        "\n",
        "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
        "g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASpkAhASZs-m"
      },
      "source": [
        "### Q1.5 Training (16 points)\n",
        "\n",
        "Note that when applying the ```criterion``` both the inputs should be on the same device. If you are defining a set of labels that you want to use in ```criterion``` you must move them to ```device``` first.\n",
        "\n",
        "Recall from the Goodfellow et al. 2014 that the following happens during the minimax game between Discriminator and the Generator.\n",
        "\n",
        "Discriminator is ascending\n",
        "$\\nabla\\frac{1}{m}\\sum_{i=1}^m\\log D(x^{(i)}) + \\log(1-D(G(z^{(i)})))$ where $D$ and $G$ denote the Districiminator and the Generator, respectively. Here, $x^{1},\\ldots,x^{m}$ is the batch of training examples (i.e., MNIST images) and $z^{(1)},\\ldots,z^{(m)}$ is a batch of random noise.\n",
        "\n",
        "Also note that for two batches $x=\\{x^{1},\\ldots,x^{m}\\}$ and $y=\\{y^{1},\\ldots,y^{m}\\}$ the BCELoss function $l(x,y)$ computes the following.\n",
        "$$l(x,y) = \\frac{1}{m}\\sum_{i=1}^m y^{(i)}\\log x^{(i)} + (1-y^{(i)})\\log (1- x^{(i)})$$\n",
        "\n",
        "The **fifth task** is to make sure that ```d_loss``` is computing the quantity that discriminator wants to ascend. This is achieved by setting ```d_loss = d_loss_real + d_loss_fake``` and letting ```d_loss_real``` to be $\\frac{1}{m}\\sum_{i=1}^m\\log D(x^{(i)})$ and ```d_loss_fake``` to be $\\frac{1}{m}\\sum_{i=1}^m \\log(1-D(G(z^{(i)})))$. Hint: ```d_loss_real``` is already implemented.\n",
        "\n",
        "We now turn to train the generator. Initially, the generator wants to descend the following $\\nabla\\frac{1}{m}\\sum_{i=1}^m \\log(1-D(G(z^{(i)})))$. However, this may affect the training of the generator specifically in the early stages of training where the discriminator can confidently reject the generated examples [Goodfellow et al., 2014]. Instead, we will require the generator to ascend $\\nabla\\frac{1}{m}\\sum_{i=1}^m \\log(D(G(z^{(i)})))$. The **sixth task** is to compute ```g_loss``` used for training the generator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RA8t-sCQZ8SG",
        "outputId": "d170d36f-369b-4906-cd06-2cf9d157f58d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/80], Step [300/600], Discriminator Loss: 0.1071, Generator Loss: 5.1463\n",
            "Epoch [1/80], Step [600/600], Discriminator Loss: 0.3309, Generator Loss: 2.9423\n",
            "Epoch [2/80], Step [300/600], Discriminator Loss: 0.0600, Generator Loss: 4.4207\n",
            "Epoch [2/80], Step [600/600], Discriminator Loss: 0.1938, Generator Loss: 3.7664\n",
            "Epoch [3/80], Step [300/600], Discriminator Loss: 0.5020, Generator Loss: 2.4576\n",
            "Epoch [3/80], Step [600/600], Discriminator Loss: 0.0621, Generator Loss: 5.0528\n",
            "Epoch [4/80], Step [300/600], Discriminator Loss: 0.1397, Generator Loss: 3.9798\n",
            "Epoch [4/80], Step [600/600], Discriminator Loss: 0.0938, Generator Loss: 6.5243\n",
            "Epoch [5/80], Step [300/600], Discriminator Loss: 0.1419, Generator Loss: 4.4510\n",
            "Epoch [5/80], Step [600/600], Discriminator Loss: 0.6368, Generator Loss: 2.7163\n",
            "Epoch [6/80], Step [300/600], Discriminator Loss: 0.6642, Generator Loss: 3.1510\n",
            "Epoch [6/80], Step [600/600], Discriminator Loss: 0.8482, Generator Loss: 1.5449\n",
            "Epoch [7/80], Step [300/600], Discriminator Loss: 0.3690, Generator Loss: 2.6802\n",
            "Epoch [7/80], Step [600/600], Discriminator Loss: 0.1536, Generator Loss: 3.6949\n",
            "Epoch [8/80], Step [300/600], Discriminator Loss: 0.3539, Generator Loss: 3.6669\n",
            "Epoch [8/80], Step [600/600], Discriminator Loss: 0.1972, Generator Loss: 4.6458\n",
            "Epoch [9/80], Step [300/600], Discriminator Loss: 0.4646, Generator Loss: 3.8903\n",
            "Epoch [9/80], Step [600/600], Discriminator Loss: 0.2278, Generator Loss: 3.5722\n",
            "Epoch [10/80], Step [300/600], Discriminator Loss: 0.4306, Generator Loss: 2.8711\n",
            "Epoch [10/80], Step [600/600], Discriminator Loss: 0.1241, Generator Loss: 5.0751\n",
            "Epoch [11/80], Step [300/600], Discriminator Loss: 0.4588, Generator Loss: 2.9776\n",
            "Epoch [11/80], Step [600/600], Discriminator Loss: 0.2351, Generator Loss: 4.0441\n",
            "Epoch [12/80], Step [300/600], Discriminator Loss: 0.1945, Generator Loss: 4.7213\n",
            "Epoch [12/80], Step [600/600], Discriminator Loss: 0.2627, Generator Loss: 4.2549\n",
            "Epoch [13/80], Step [300/600], Discriminator Loss: 0.4203, Generator Loss: 4.6843\n",
            "Epoch [13/80], Step [600/600], Discriminator Loss: 0.2726, Generator Loss: 5.2366\n",
            "Epoch [14/80], Step [300/600], Discriminator Loss: 0.3032, Generator Loss: 4.6394\n",
            "Epoch [14/80], Step [600/600], Discriminator Loss: 0.2233, Generator Loss: 4.9356\n",
            "Epoch [15/80], Step [300/600], Discriminator Loss: 0.2528, Generator Loss: 3.9196\n",
            "Epoch [15/80], Step [600/600], Discriminator Loss: 0.1441, Generator Loss: 4.3948\n",
            "Epoch [16/80], Step [300/600], Discriminator Loss: 0.3165, Generator Loss: 3.5649\n",
            "Epoch [16/80], Step [600/600], Discriminator Loss: 0.2412, Generator Loss: 4.8617\n",
            "Epoch [17/80], Step [300/600], Discriminator Loss: 0.3585, Generator Loss: 4.7499\n",
            "Epoch [17/80], Step [600/600], Discriminator Loss: 0.3374, Generator Loss: 4.2811\n",
            "Epoch [18/80], Step [300/600], Discriminator Loss: 0.2225, Generator Loss: 4.1256\n",
            "Epoch [18/80], Step [600/600], Discriminator Loss: 0.3694, Generator Loss: 3.2260\n",
            "Epoch [19/80], Step [300/600], Discriminator Loss: 0.3984, Generator Loss: 2.9767\n",
            "Epoch [19/80], Step [600/600], Discriminator Loss: 0.3014, Generator Loss: 5.5724\n",
            "Epoch [20/80], Step [300/600], Discriminator Loss: 0.5825, Generator Loss: 4.3981\n",
            "Epoch [20/80], Step [600/600], Discriminator Loss: 0.4783, Generator Loss: 4.0397\n",
            "Epoch [21/80], Step [300/600], Discriminator Loss: 0.5195, Generator Loss: 2.5143\n",
            "Epoch [21/80], Step [600/600], Discriminator Loss: 0.5258, Generator Loss: 2.4681\n",
            "Epoch [22/80], Step [300/600], Discriminator Loss: 0.3513, Generator Loss: 3.1624\n",
            "Epoch [22/80], Step [600/600], Discriminator Loss: 0.2530, Generator Loss: 3.6115\n",
            "Epoch [23/80], Step [300/600], Discriminator Loss: 0.3549, Generator Loss: 3.5639\n",
            "Epoch [23/80], Step [600/600], Discriminator Loss: 0.4821, Generator Loss: 4.0944\n",
            "Epoch [24/80], Step [300/600], Discriminator Loss: 0.1663, Generator Loss: 4.0436\n",
            "Epoch [24/80], Step [600/600], Discriminator Loss: 0.5332, Generator Loss: 2.0694\n",
            "Epoch [25/80], Step [300/600], Discriminator Loss: 0.3465, Generator Loss: 4.2531\n",
            "Epoch [25/80], Step [600/600], Discriminator Loss: 0.4745, Generator Loss: 2.9608\n",
            "Epoch [26/80], Step [300/600], Discriminator Loss: 0.4121, Generator Loss: 4.1618\n",
            "Epoch [26/80], Step [600/600], Discriminator Loss: 0.3714, Generator Loss: 3.6730\n",
            "Epoch [27/80], Step [300/600], Discriminator Loss: 0.4246, Generator Loss: 4.0611\n",
            "Epoch [27/80], Step [600/600], Discriminator Loss: 0.5891, Generator Loss: 2.4960\n",
            "Epoch [28/80], Step [300/600], Discriminator Loss: 0.3134, Generator Loss: 3.0140\n",
            "Epoch [28/80], Step [600/600], Discriminator Loss: 0.4832, Generator Loss: 2.5297\n",
            "Epoch [29/80], Step [300/600], Discriminator Loss: 0.5571, Generator Loss: 2.6876\n",
            "Epoch [29/80], Step [600/600], Discriminator Loss: 0.3873, Generator Loss: 2.9834\n",
            "Epoch [30/80], Step [300/600], Discriminator Loss: 0.4998, Generator Loss: 2.7009\n",
            "Epoch [30/80], Step [600/600], Discriminator Loss: 0.5790, Generator Loss: 2.7112\n",
            "Epoch [31/80], Step [300/600], Discriminator Loss: 0.4230, Generator Loss: 2.4563\n",
            "Epoch [31/80], Step [600/600], Discriminator Loss: 0.3916, Generator Loss: 2.6139\n",
            "Epoch [32/80], Step [300/600], Discriminator Loss: 0.5990, Generator Loss: 2.4622\n",
            "Epoch [32/80], Step [600/600], Discriminator Loss: 0.5203, Generator Loss: 3.2883\n",
            "Epoch [33/80], Step [300/600], Discriminator Loss: 0.5886, Generator Loss: 2.4163\n",
            "Epoch [33/80], Step [600/600], Discriminator Loss: 0.4198, Generator Loss: 3.4140\n",
            "Epoch [34/80], Step [300/600], Discriminator Loss: 0.5174, Generator Loss: 2.3986\n",
            "Epoch [34/80], Step [600/600], Discriminator Loss: 0.4494, Generator Loss: 3.5340\n",
            "Epoch [35/80], Step [300/600], Discriminator Loss: 0.4243, Generator Loss: 3.1746\n",
            "Epoch [35/80], Step [600/600], Discriminator Loss: 0.3785, Generator Loss: 2.1665\n",
            "Epoch [36/80], Step [300/600], Discriminator Loss: 0.5770, Generator Loss: 2.1451\n",
            "Epoch [36/80], Step [600/600], Discriminator Loss: 0.6075, Generator Loss: 3.0046\n",
            "Epoch [37/80], Step [300/600], Discriminator Loss: 0.5286, Generator Loss: 2.1192\n",
            "Epoch [37/80], Step [600/600], Discriminator Loss: 0.5807, Generator Loss: 2.3782\n",
            "Epoch [38/80], Step [300/600], Discriminator Loss: 0.6942, Generator Loss: 1.6603\n",
            "Epoch [38/80], Step [600/600], Discriminator Loss: 0.9128, Generator Loss: 1.7416\n",
            "Epoch [39/80], Step [300/600], Discriminator Loss: 0.5945, Generator Loss: 2.0515\n",
            "Epoch [39/80], Step [600/600], Discriminator Loss: 0.5922, Generator Loss: 2.3681\n",
            "Epoch [40/80], Step [300/600], Discriminator Loss: 0.5706, Generator Loss: 1.9808\n",
            "Epoch [40/80], Step [600/600], Discriminator Loss: 0.5641, Generator Loss: 2.2059\n",
            "Epoch [41/80], Step [300/600], Discriminator Loss: 0.5519, Generator Loss: 1.9867\n",
            "Epoch [41/80], Step [600/600], Discriminator Loss: 0.6680, Generator Loss: 1.9071\n",
            "Epoch [42/80], Step [300/600], Discriminator Loss: 0.7390, Generator Loss: 2.6689\n",
            "Epoch [42/80], Step [600/600], Discriminator Loss: 0.8945, Generator Loss: 2.6984\n",
            "Epoch [43/80], Step [300/600], Discriminator Loss: 0.6475, Generator Loss: 2.2995\n",
            "Epoch [43/80], Step [600/600], Discriminator Loss: 0.6011, Generator Loss: 2.3748\n",
            "Epoch [44/80], Step [300/600], Discriminator Loss: 0.5426, Generator Loss: 2.2274\n",
            "Epoch [44/80], Step [600/600], Discriminator Loss: 0.7631, Generator Loss: 2.5269\n",
            "Epoch [45/80], Step [300/600], Discriminator Loss: 0.6138, Generator Loss: 2.4750\n",
            "Epoch [45/80], Step [600/600], Discriminator Loss: 0.6984, Generator Loss: 2.7053\n",
            "Epoch [46/80], Step [300/600], Discriminator Loss: 0.7666, Generator Loss: 3.0271\n",
            "Epoch [46/80], Step [600/600], Discriminator Loss: 0.6699, Generator Loss: 2.4369\n",
            "Epoch [47/80], Step [300/600], Discriminator Loss: 0.5887, Generator Loss: 2.6095\n",
            "Epoch [47/80], Step [600/600], Discriminator Loss: 0.8093, Generator Loss: 2.5923\n",
            "Epoch [48/80], Step [300/600], Discriminator Loss: 0.6934, Generator Loss: 1.6549\n",
            "Epoch [48/80], Step [600/600], Discriminator Loss: 0.7153, Generator Loss: 1.7099\n",
            "Epoch [49/80], Step [300/600], Discriminator Loss: 0.7830, Generator Loss: 2.1275\n",
            "Epoch [49/80], Step [600/600], Discriminator Loss: 0.6815, Generator Loss: 2.3425\n",
            "Epoch [50/80], Step [300/600], Discriminator Loss: 0.9468, Generator Loss: 1.5918\n",
            "Epoch [50/80], Step [600/600], Discriminator Loss: 0.6251, Generator Loss: 2.2778\n",
            "Epoch [51/80], Step [300/600], Discriminator Loss: 0.8095, Generator Loss: 1.7224\n",
            "Epoch [51/80], Step [600/600], Discriminator Loss: 0.6157, Generator Loss: 2.0982\n",
            "Epoch [52/80], Step [300/600], Discriminator Loss: 0.7481, Generator Loss: 1.9445\n",
            "Epoch [52/80], Step [600/600], Discriminator Loss: 0.6631, Generator Loss: 2.2232\n",
            "Epoch [53/80], Step [300/600], Discriminator Loss: 0.7328, Generator Loss: 1.9209\n",
            "Epoch [53/80], Step [600/600], Discriminator Loss: 0.8038, Generator Loss: 1.4362\n",
            "Epoch [54/80], Step [300/600], Discriminator Loss: 0.5320, Generator Loss: 2.6017\n",
            "Epoch [54/80], Step [600/600], Discriminator Loss: 0.9678, Generator Loss: 2.0390\n",
            "Epoch [55/80], Step [300/600], Discriminator Loss: 0.7406, Generator Loss: 2.0810\n",
            "Epoch [55/80], Step [600/600], Discriminator Loss: 0.6922, Generator Loss: 1.7125\n",
            "Epoch [56/80], Step [300/600], Discriminator Loss: 0.7942, Generator Loss: 1.9242\n",
            "Epoch [56/80], Step [600/600], Discriminator Loss: 0.6521, Generator Loss: 1.7918\n",
            "Epoch [57/80], Step [300/600], Discriminator Loss: 0.6937, Generator Loss: 2.3265\n",
            "Epoch [57/80], Step [600/600], Discriminator Loss: 0.6319, Generator Loss: 2.4639\n",
            "Epoch [58/80], Step [300/600], Discriminator Loss: 0.8457, Generator Loss: 2.0322\n",
            "Epoch [58/80], Step [600/600], Discriminator Loss: 0.8005, Generator Loss: 2.0413\n",
            "Epoch [59/80], Step [300/600], Discriminator Loss: 0.5630, Generator Loss: 1.8847\n",
            "Epoch [59/80], Step [600/600], Discriminator Loss: 0.7972, Generator Loss: 2.0498\n",
            "Epoch [60/80], Step [300/600], Discriminator Loss: 0.5145, Generator Loss: 2.3418\n",
            "Epoch [60/80], Step [600/600], Discriminator Loss: 0.7241, Generator Loss: 1.9990\n",
            "Epoch [61/80], Step [300/600], Discriminator Loss: 1.0463, Generator Loss: 1.8663\n",
            "Epoch [61/80], Step [600/600], Discriminator Loss: 0.8860, Generator Loss: 1.6317\n",
            "Epoch [62/80], Step [300/600], Discriminator Loss: 0.8744, Generator Loss: 1.8765\n",
            "Epoch [62/80], Step [600/600], Discriminator Loss: 0.6440, Generator Loss: 1.9186\n",
            "Epoch [63/80], Step [300/600], Discriminator Loss: 0.7111, Generator Loss: 1.7039\n",
            "Epoch [63/80], Step [600/600], Discriminator Loss: 0.8077, Generator Loss: 2.4946\n",
            "Epoch [64/80], Step [300/600], Discriminator Loss: 0.7936, Generator Loss: 2.0028\n",
            "Epoch [64/80], Step [600/600], Discriminator Loss: 0.6932, Generator Loss: 1.9018\n",
            "Epoch [65/80], Step [300/600], Discriminator Loss: 0.5515, Generator Loss: 2.6589\n",
            "Epoch [65/80], Step [600/600], Discriminator Loss: 0.6757, Generator Loss: 2.1889\n",
            "Epoch [66/80], Step [300/600], Discriminator Loss: 0.7674, Generator Loss: 1.6358\n",
            "Epoch [66/80], Step [600/600], Discriminator Loss: 0.6195, Generator Loss: 2.6869\n",
            "Epoch [67/80], Step [300/600], Discriminator Loss: 0.9669, Generator Loss: 1.2006\n",
            "Epoch [67/80], Step [600/600], Discriminator Loss: 0.7753, Generator Loss: 1.8118\n",
            "Epoch [68/80], Step [300/600], Discriminator Loss: 0.7186, Generator Loss: 1.9318\n",
            "Epoch [68/80], Step [600/600], Discriminator Loss: 0.6723, Generator Loss: 1.8359\n",
            "Epoch [69/80], Step [300/600], Discriminator Loss: 0.8577, Generator Loss: 1.6964\n",
            "Epoch [69/80], Step [600/600], Discriminator Loss: 0.8564, Generator Loss: 1.6796\n",
            "Epoch [70/80], Step [300/600], Discriminator Loss: 0.8748, Generator Loss: 2.1423\n",
            "Epoch [70/80], Step [600/600], Discriminator Loss: 0.8965, Generator Loss: 1.7280\n",
            "Epoch [71/80], Step [300/600], Discriminator Loss: 0.8279, Generator Loss: 1.6851\n",
            "Epoch [71/80], Step [600/600], Discriminator Loss: 0.8962, Generator Loss: 1.9833\n",
            "Epoch [72/80], Step [300/600], Discriminator Loss: 0.8042, Generator Loss: 1.9731\n",
            "Epoch [72/80], Step [600/600], Discriminator Loss: 0.7993, Generator Loss: 1.8908\n",
            "Epoch [73/80], Step [300/600], Discriminator Loss: 0.7687, Generator Loss: 1.9093\n",
            "Epoch [73/80], Step [600/600], Discriminator Loss: 0.7616, Generator Loss: 1.5569\n",
            "Epoch [74/80], Step [300/600], Discriminator Loss: 0.9051, Generator Loss: 1.6831\n",
            "Epoch [74/80], Step [600/600], Discriminator Loss: 0.8110, Generator Loss: 1.5897\n",
            "Epoch [75/80], Step [300/600], Discriminator Loss: 0.8686, Generator Loss: 1.7293\n",
            "Epoch [75/80], Step [600/600], Discriminator Loss: 0.7325, Generator Loss: 2.1845\n",
            "Epoch [76/80], Step [300/600], Discriminator Loss: 0.9265, Generator Loss: 1.6913\n",
            "Epoch [76/80], Step [600/600], Discriminator Loss: 0.8309, Generator Loss: 2.5844\n",
            "Epoch [77/80], Step [300/600], Discriminator Loss: 0.7762, Generator Loss: 2.0291\n",
            "Epoch [77/80], Step [600/600], Discriminator Loss: 0.7253, Generator Loss: 2.0366\n",
            "Epoch [78/80], Step [300/600], Discriminator Loss: 1.1019, Generator Loss: 1.5556\n",
            "Epoch [78/80], Step [600/600], Discriminator Loss: 0.8213, Generator Loss: 1.4691\n",
            "Epoch [79/80], Step [300/600], Discriminator Loss: 0.8709, Generator Loss: 1.7158\n",
            "Epoch [79/80], Step [600/600], Discriminator Loss: 0.8150, Generator Loss: 1.7747\n",
            "Epoch [80/80], Step [300/600], Discriminator Loss: 0.9397, Generator Loss: 1.7925\n",
            "Epoch [80/80], Step [600/600], Discriminator Loss: 0.6983, Generator Loss: 1.5767\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 80\n",
        "total_step = len(data_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, _) in enumerate(data_loader):\n",
        "        real_images = images.to(device)\n",
        "\n",
        "        # Training discriminator\n",
        "        discriminator.zero_grad()\n",
        "        real_outputs = discriminator(real_images)\n",
        "        real_labels = torch.ones(real_images.size(0), 1).to(device)\n",
        "        d_loss_real = criterion(real_outputs, real_labels)\n",
        "\n",
        "        #ToDo: Compute d_loss_fake\n",
        "        noise = torch.randn(real_images.size(0), n_dim).to(device)\n",
        "        fake_images = generator(noise)\n",
        "        fake_outputs = discriminator(fake_images.detach())\n",
        "        fake_labels = torch.zeros(fake_images.size(0), 1).to(device)\n",
        "        d_loss_fake = criterion(fake_outputs, fake_labels)\n",
        "\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        # Training generator\n",
        "        generator.zero_grad()\n",
        "\n",
        "        output = discriminator(fake_images)\n",
        "        real_labels = torch.ones(fake_images.size(0), 1).to(device)\n",
        "        #ToDo: compute g_loss\n",
        "        g_loss = criterion(output, real_labels)\n",
        "\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        if (i+1) % 300 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], '\n",
        "                  f'Discriminator Loss: {d_loss.item():.4f}, '\n",
        "                  f'Generator Loss: {g_loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3CGaQzDDRg8"
      },
      "source": [
        "### Q1.6 Plotting the results (5 points)\n",
        "The **seventh and final task** for this question is to try the trained generator. Generate 4 random vectors, apply the generator on them, and plot the resulting image in a single 2 by 2 plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "lvUJnaqSJCPM",
        "outputId": "934b0079-e304-4d8f-8bc6-c4b12d201819"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "# Your code here\n",
        "  noise = torch.randn(4, n_dim).to(device)\n",
        "  fake_images = generator(noise)\n",
        "\n",
        "  fake_images = fake_images.view(-1, 28, 28).cpu().numpy()\n",
        "\n",
        "  fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
        "  axs = axs.ravel()\n",
        "  for i in range(4):\n",
        "      axs[i].imshow(fake_images[i], cmap='gray')\n",
        "      axs[i].axis('off')\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifoL2LfccMKA"
      },
      "source": [
        "## Question 2 - Transformers (65 points)\n",
        "In this question we will be implementing a transformer architecture and apply it on a simple artificially generated dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtZX21CEcu6p"
      },
      "source": [
        "### Q2.1 Multi-Head Attention (9 points)\n",
        "\n",
        "The attention layer works on batches of training data where each training data itself is a sequence of words/token with length at most ```seq_length``` and each word is embedded into a ```d_embedding``` dimensional vector, e.g., later when we run the transformer on the artificial dataset, the inputs to the transformer and attention layer have shapes of ```[batch_size,seq_length,d_embedding]```. Recall from the lecture that a multi-head attention layer consists of ```n_heads``` self-attention heads. The input matrices Query Q, Key K, and Value V to multi-attention head are passed through linear layers $W_Q$, $W_K$, and $W_V$ which are trainable parameters. The multi-head attention will divide the embedding vector of the input into ```n_heads``` vector of size ```d_k``` and use each as an attention head. This is particularly done by the ```make_heads``` function which transforms the input to an output of shape ```[batch_size, n_heads, seq_length, d_k]```. The multi-head attention then computes the probabilities using the input values, which is implemented by the ```compute_attention_probs``` function. It then joins them using the ```join_heads``` function and finally passes them through an output linear layer $W_O$. The **first task** is to implement the ```forward``` function of Multi-Head attention using the functions already implemented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pE_j8e5u4aLv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_embedding, n_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_embedding % n_heads == 0\n",
        "\n",
        "        self.d_embedding = d_embedding\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_embedding // n_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_embedding, d_embedding)\n",
        "        self.W_k = nn.Linear(d_embedding, d_embedding)\n",
        "        self.W_v = nn.Linear(d_embedding, d_embedding)\n",
        "        self.W_o = nn.Linear(d_embedding, d_embedding)\n",
        "\n",
        "    def compute_attention_probs(self, Q, K, V, mask=None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def make_heads(self, x):\n",
        "        batch_size, seq_length, d_embedding = x.size()\n",
        "        y=x.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        return x.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def join_heads(self, x):\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_embedding)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "       #ToDo: Complete code\n",
        "       Q_proj = self.W_q(Q)\n",
        "       K_proj = self.W_k(K)\n",
        "       V_proj = self.W_v(V)\n",
        "\n",
        "       Q_heads = self.make_heads(Q_proj)\n",
        "       K_heads = self.make_heads(K_proj)\n",
        "       V_heads = self.make_heads(V_proj)\n",
        "\n",
        "       attn_output = self.compute_attention_probs(Q_heads, K_heads, V_heads, mask)\n",
        "\n",
        "       output = self.join_heads(attn_output)\n",
        "       output = self.W_o(output)\n",
        "\n",
        "       return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT0KE6fB6497"
      },
      "source": [
        "### Q2.2 Feed-forward layer (2 points)\n",
        "The output of the multi-head attention layer is used as an input to the feed-forward layer in the encoder. This layer is implemented in the following class. The **second task** is to complete the ```forward``` function of this class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "InGgN9XN7muD"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_embedding, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_embedding, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_embedding)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        #ToDO: Complete code\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaZ1PCFJ9DXf"
      },
      "source": [
        "### Q2.3 Encoder Block (9 points)\n",
        "\n",
        "We now have all the necessary classes to build the Encoder block. The encoder block gets as input a batch of data. It first passes the input through the multi-attention layer. It then applies 1) a droput layer, 2) adds the input to the output of droupout layer as a residual, 3) applies a layer normalization, 4) and passes the output to the feed-forward layer. Finally, it applies 1) a droput layer, 2) adds the input to the output of droupout layer as a residual, and 3) Applies a layer normalization. The **third task** is to complete the ```forward``` function of this class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s0MnjetG_Zum"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_embedding, n_heads, d_ff, dropout):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_embedding, n_heads)\n",
        "        self.feed_forward = FeedForward(d_embedding, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_embedding)\n",
        "        self.norm2 = nn.LayerNorm(d_embedding)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        #ToDo: Complete code\n",
        "        \"\"\"\n",
        "        Returns: The output of encoder on a given input batch x\n",
        "        \"\"\"\n",
        "        attn_output = self.self_attention(x, x, x, mask)\n",
        "        attn_output = self.dropout(attn_output)\n",
        "\n",
        "        x = self.norm1(x + attn_output)\n",
        "\n",
        "        ff_output = self.feed_forward(x)\n",
        "        ff_output = self.dropout(ff_output)\n",
        "\n",
        "        x = self.norm2(x + ff_output)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jru9E9dZ_aKv"
      },
      "source": [
        "### Q2.4 Decoder Block (20 points)\n",
        "\n",
        "In this part we will build the decoder block. The **fourth task** is to implement the DecoderBlock. You can refer to the lecture notes to find the architecture of the decoder block including the multi-attention layers, add and norm layers, and the feedforward layer. You can also add a *dropout* layer to the final add and norm layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HCX_D3xoFYJo"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_embedding, n_heads, d_ff, dropout):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           self_attention: decoder's multi-head attention\n",
        "           cross_attention: The multi-head attention layer between the encoder and the decoder\n",
        "           feed_forward: feed-forward layer\n",
        "           mask: mask to be given for multi head attention\n",
        "           norm1: First Normalization layer\n",
        "           norm2: Second Normalization layer\n",
        "           norm3: Third Normalization layer\n",
        "           dropout: Final dropout layer\n",
        "        \"\"\"\n",
        "\n",
        "        self.self_attention = MultiHeadAttention(d_embedding, n_heads)\n",
        "        self.cross_attention = MultiHeadAttention(d_embedding, n_heads)\n",
        "        self.feed_forward = FeedForward(d_embedding, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_embedding)\n",
        "        self.norm2 = nn.LayerNorm(d_embedding)\n",
        "        self.norm3 = nn.LayerNorm(d_embedding)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        \"\"\"\n",
        "        Returns: The output of decoder on a given input batch x\n",
        "        \"\"\"\n",
        "\n",
        "        attn_output = self.self_attention(x, x, x, tgt_mask)\n",
        "        attn_output = self.dropout(attn_output)\n",
        "\n",
        "        x = self.norm1(x + attn_output)\n",
        "\n",
        "        cross_attn_output = self.cross_attention(x, enc_output, enc_output, src_mask)\n",
        "        cross_attn_output = self.dropout(cross_attn_output)\n",
        "\n",
        "        x = self.norm2(x + cross_attn_output)\n",
        "\n",
        "        ff_output = self.feed_forward(x)\n",
        "        ff_output = self.dropout(ff_output)\n",
        "\n",
        "        x = self.norm3(x + ff_output)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyH9ROxRI252"
      },
      "source": [
        "### Q2.5 Transformer Block (20 points)\n",
        "\n",
        "Finally, we use the classes defined for EncoderBlock and DecoderBlock to build the TransformerBlock. The **fifth task** is to complete the ```forward``` function of this class. Note that ```num_layers``` indicates the number of encoder and decoder layers in the transformer. You can refer to the final architecture of the Transformer from the lectures. You can also include a dropout layer after embedding the source and target inputs. Remember to apply positional encoding to the input batch before passing it to the transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2Sz2JAuwmnNn"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_embedding, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_embedding)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_embedding, 2).float() * -(math.log(10000.0) / d_embedding))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_embedding, n_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_embedding)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_embedding)\n",
        "        self.positional_encoding = PositionalEncoding(d_embedding, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderBlock(d_embedding, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderBlock(d_embedding, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_embedding, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Masking the input; the outputs returned by this function should be passed to encoder and decoder layers\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        #ToDo: complete the code\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        \"\"\"\n",
        "        Returns: The output of transformer on a given input batch x\n",
        "        \"\"\"\n",
        "        src = self.encoder_embedding(src)\n",
        "        src = self.positional_encoding(src)\n",
        "        src = self.dropout(src)\n",
        "\n",
        "        for layer in self.encoder_layers:\n",
        "            src = layer(src, src_mask)\n",
        "\n",
        "        tgt = self.decoder_embedding(tgt)\n",
        "        tgt = self.positional_encoding(tgt)\n",
        "        tgt = self.dropout(tgt)\n",
        "\n",
        "        for layer in self.decoder_layers:\n",
        "            tgt = layer(tgt, src, src_mask, tgt_mask)\n",
        "\n",
        "\n",
        "        output = self.fc(tgt)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGtuTlqWms4B"
      },
      "source": [
        "### Q2.6 Training (5 points)\n",
        "We can now train the model on an imaginary randomly generated dataset. The **sixth task** is to complete the training code and plot the training loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "id": "94xoG6QF5gnv",
        "outputId": "da3632cd-c9a0-4a5e-923f-df7d07275640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/25], Loss: 8.6861\n",
            "Epoch [2/25], Loss: 8.5543\n",
            "Epoch [3/25], Loss: 8.4790\n",
            "Epoch [4/25], Loss: 8.4289\n",
            "Epoch [5/25], Loss: 8.3707\n",
            "Epoch [6/25], Loss: 8.3070\n",
            "Epoch [7/25], Loss: 8.2345\n",
            "Epoch [8/25], Loss: 8.1482\n",
            "Epoch [9/25], Loss: 8.0705\n",
            "Epoch [10/25], Loss: 7.9879\n",
            "Epoch [11/25], Loss: 7.9059\n",
            "Epoch [12/25], Loss: 7.8287\n",
            "Epoch [13/25], Loss: 7.7497\n",
            "Epoch [14/25], Loss: 7.6646\n",
            "Epoch [15/25], Loss: 7.5807\n",
            "Epoch [16/25], Loss: 7.4970\n",
            "Epoch [17/25], Loss: 7.4108\n",
            "Epoch [18/25], Loss: 7.3329\n",
            "Epoch [19/25], Loss: 7.2516\n",
            "Epoch [20/25], Loss: 7.1705\n",
            "Epoch [21/25], Loss: 7.0951\n",
            "Epoch [22/25], Loss: 7.0087\n",
            "Epoch [23/25], Loss: 6.9296\n",
            "Epoch [24/25], Loss: 6.8578\n",
            "Epoch [25/25], Loss: 6.7802\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "src_vocab_size = 5000\n",
        "tgt_vocab_size = 5000\n",
        "d_embedding = 512\n",
        "n_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "\n",
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_embedding, n_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "\n",
        "# Generate random sample data\n",
        "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "transformer.train()\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(25):\n",
        "   #ToDo: complete the code\n",
        "\n",
        "   optimizer.zero_grad()\n",
        "   output = transformer(src_data, tgt_data[:, :-1])\n",
        "\n",
        "   output = output.view(-1, tgt_vocab_size)\n",
        "   tgt = tgt_data[:, 1:].contiguous().view(-1)\n",
        "\n",
        "   loss = criterion(output, tgt)\n",
        "   losses.append(loss.item())\n",
        "\n",
        "   loss.backward()\n",
        "   optimizer.step()\n",
        "\n",
        "   print(f\"Epoch [{epoch+1}/25], Loss: {loss.item():.4f}\")\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}